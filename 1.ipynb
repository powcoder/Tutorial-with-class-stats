{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (50 pts) Part 1: Missing Data Analysis and Dimensionality Reduction\n",
    "\n",
    "To answer the first part of Lab1, use the data as described in the Jupyter tutorial presented in class and derived from the following paper \n",
    "\n",
    "> Moosavi, Sobhan, Mohammad Hossein Samavatian, Srinivasan Parthasarathy, Radu Teodorescu, and Rajiv Ramnath. _\"Accident Risk Prediction based on Heterogeneous Sparse Data: New Dataset and Insights.\"_ In proceedings of the 27th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems, ACM, 2019. (https://arxiv.org/abs/1909.09638).\n",
    "\n",
    "The data can also be found at https://www.kaggle.com/sobhanmoosavi/us-accidents. \n",
    "\n",
    ">### _Objective of this Lab:_\n",
    "> * _Inform you about best practice pertaining missing data._\n",
    "> * _Reducing the number of attributes._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (25 pts) Question 1: Missing Data \n",
    "Missing data typically occurs when no data value is observed for any given set of attributes. This can have a significant impact on the inference that can be derived from the data and has to be addressed carefully to minimize said impact.  The _\"accident risk\"_ dataset has a certain amount of missing data. There is a need to address this problem. Fill in missing values for every  data column with continuous data using the following three strategies:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Source</th>\n",
       "      <th>TMC</th>\n",
       "      <th>Severity</th>\n",
       "      <th>Start_Time</th>\n",
       "      <th>End_Time</th>\n",
       "      <th>Start_Lat</th>\n",
       "      <th>Start_Lng</th>\n",
       "      <th>End_Lat</th>\n",
       "      <th>End_Lng</th>\n",
       "      <th>...</th>\n",
       "      <th>Roundabout</th>\n",
       "      <th>Station</th>\n",
       "      <th>Stop</th>\n",
       "      <th>Traffic_Calming</th>\n",
       "      <th>Traffic_Signal</th>\n",
       "      <th>Turning_Loop</th>\n",
       "      <th>Sunrise_Sunset</th>\n",
       "      <th>Civil_Twilight</th>\n",
       "      <th>Nautical_Twilight</th>\n",
       "      <th>Astronomical_Twilight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A-603309</td>\n",
       "      <td>MapQuest</td>\n",
       "      <td>241.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2019-02-15 16:56:38</td>\n",
       "      <td>2019-02-15 17:26:16</td>\n",
       "      <td>34.725163</td>\n",
       "      <td>-86.596359</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Day</td>\n",
       "      <td>Day</td>\n",
       "      <td>Day</td>\n",
       "      <td>Day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A-676455</td>\n",
       "      <td>MapQuest</td>\n",
       "      <td>201.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2019-01-26 15:54:00</td>\n",
       "      <td>2019-01-26 16:25:00</td>\n",
       "      <td>32.883579</td>\n",
       "      <td>-80.019722</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Day</td>\n",
       "      <td>Day</td>\n",
       "      <td>Day</td>\n",
       "      <td>Day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A-2170526</td>\n",
       "      <td>Bing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>2018-01-06 10:51:40</td>\n",
       "      <td>2018-01-06 16:51:40</td>\n",
       "      <td>39.979172</td>\n",
       "      <td>-82.983870</td>\n",
       "      <td>39.99384</td>\n",
       "      <td>-82.98502</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Day</td>\n",
       "      <td>Day</td>\n",
       "      <td>Day</td>\n",
       "      <td>Day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A-1162086</td>\n",
       "      <td>MapQuest</td>\n",
       "      <td>201.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2018-05-25 16:12:02</td>\n",
       "      <td>2018-05-25 17:11:49</td>\n",
       "      <td>34.857208</td>\n",
       "      <td>-82.256157</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>Day</td>\n",
       "      <td>Day</td>\n",
       "      <td>Day</td>\n",
       "      <td>Day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A-309058</td>\n",
       "      <td>MapQuest</td>\n",
       "      <td>201.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-10-26 19:42:11</td>\n",
       "      <td>2016-10-26 20:26:58</td>\n",
       "      <td>47.662689</td>\n",
       "      <td>-117.357658</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>Night</td>\n",
       "      <td>Night</td>\n",
       "      <td>Night</td>\n",
       "      <td>Night</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID    Source    TMC  Severity           Start_Time  \\\n",
       "0   A-603309  MapQuest  241.0         2  2019-02-15 16:56:38   \n",
       "1   A-676455  MapQuest  201.0         2  2019-01-26 15:54:00   \n",
       "2  A-2170526      Bing    NaN         4  2018-01-06 10:51:40   \n",
       "3  A-1162086  MapQuest  201.0         2  2018-05-25 16:12:02   \n",
       "4   A-309058  MapQuest  201.0         2  2016-10-26 19:42:11   \n",
       "\n",
       "              End_Time  Start_Lat   Start_Lng   End_Lat   End_Lng  \\\n",
       "0  2019-02-15 17:26:16  34.725163  -86.596359       NaN       NaN   \n",
       "1  2019-01-26 16:25:00  32.883579  -80.019722       NaN       NaN   \n",
       "2  2018-01-06 16:51:40  39.979172  -82.983870  39.99384 -82.98502   \n",
       "3  2018-05-25 17:11:49  34.857208  -82.256157       NaN       NaN   \n",
       "4  2016-10-26 20:26:58  47.662689 -117.357658       NaN       NaN   \n",
       "\n",
       "           ...           Roundabout Station   Stop Traffic_Calming  \\\n",
       "0          ...                False   False  False           False   \n",
       "1          ...                False   False  False           False   \n",
       "2          ...                False   False  False           False   \n",
       "3          ...                False   False  False           False   \n",
       "4          ...                False   False  False           False   \n",
       "\n",
       "  Traffic_Signal Turning_Loop Sunrise_Sunset Civil_Twilight Nautical_Twilight  \\\n",
       "0          False        False            Day            Day               Day   \n",
       "1          False        False            Day            Day               Day   \n",
       "2          False        False            Day            Day               Day   \n",
       "3           True        False            Day            Day               Day   \n",
       "4           True        False          Night          Night             Night   \n",
       "\n",
       "  Astronomical_Twilight  \n",
       "0                   Day  \n",
       "1                   Day  \n",
       "2                   Day  \n",
       "3                   Day  \n",
       "4                 Night  \n",
       "\n",
       "[5 rows x 49 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# be sure to make use of the full dataset.\n",
    "df = pd.read_csv(\"./data/US_Accidents_May19.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in missing values for every continuous data column using the following three strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (10 pts) Q 1.1: Use the mean to fill the missing values for any given attribute.\n",
    "\n",
    "First collect the mean value of all the observations of each attribute with missing data. Then use the mean value (calculated per attribute) to fill in for  the missing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (15 pts) Q 1.2: Randomly sample based on the probability distribution of the given column data.\n",
    "\n",
    "Generate and fill missing values using random samples from the probability distribution estimated from the observed values for each column. Verify your estimated pdf by comparing the histogram of observed values vs estimated values. Remember to fill all the columns containing missing data.\n",
    "\n",
    "> Steps to generate random samples by estimating the probability distribution:\n",
    "> * First generate the probability distribution of the attribute using the existing observations. \n",
    ">   - Draw Histogram of the observed values\n",
    ">   - Estimate the distribution using [KDE](https://en.wikipedia.org/wiki/Kernel_density_estimation)\n",
    "> * Using the pdf find the cdf (make sure the cdf is normalized to values 0 to 1).\n",
    "> * Generate the inverse cdf function, and pass a random value \\[0,1\\] from a uniform distribution to the inverse function to get random samples from our estimated pdf.\n",
    "> \n",
    "> i.e.,    \n",
    ">      $$r = \\mathit{Uniform}(0, 1)$$\n",
    ">      $$x = \\mathit{cdf}^{-1}(r)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Bonus - 15 pts) Q 1.3: Use linear regression using other continuous columns fill in the missing data.\n",
    "\n",
    "Make sure to use multiple  attributes (columns) that do not contain too missing values ($< 5\\%$) as training features or attributes. Let us say that you select four training attributes each with some possible missing values. Now, select only those rows which are fully populated for each of the four attributes. Next, use multivariate regression to predict the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# code here\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "# create 2d array X here\n",
    "\n",
    "\n",
    "# X = All the other predictors as a 2d array. \n",
    "# where, rows are the samples and columns are training features,\n",
    "# or attributes that have < 5% missing values\n",
    "\n",
    "\n",
    "# create 1d array y here\n",
    "\n",
    "# y = Missing data column that you wish to fill. It must\n",
    "# only contain the observed values for training.\n",
    "\n",
    "\n",
    "# train the model\n",
    "reg = LinearRegression().fit(X, y)\n",
    "\n",
    "# predict the values using the same set of training features.\n",
    "# here the rows are samples that contain missing values in the y column,\n",
    "# and the columns are the same training features for those samples.\n",
    "y_miss = reg.predict(X_miss)\n",
    "\n",
    "#now repeat this for all columns containing missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (25 points) Question 2: Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often data is collected with a certain design and purpose. After the collection is over, it is often determined that certain attributes do not have any impact on the sought inferences and/or predictions. It is therefore of value (storage, computing time, etc.) to reduce the amount of time. Think of compression schemes. We ask you to do the same for the accident risk data. There are many methods to reduce data size. For this laboratory, we ask you to use a dimensionality reduction called Principal Component Analysis or PCA. \n",
    "\n",
    "Conduct dimensionality reduction using PCA on the two datasets generated after answering Question 1.1 and Question 1.2 respectively. Follow these steps:\n",
    "\n",
    "\n",
    "#### 1. Conduct exploratory analysis in the following manner\n",
    "\n",
    "\n",
    "##### 1.1. Consider only the columns with continuous data (there should be 10 weather related columns and nine should be continuous)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2. Choose five attributes you think are important to predict severity. Please feel to consult the paper if necessary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.3. Plot scatter plot matrix for all the five attributes for 1000 randomly selected points.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.4. Comment on what you observe.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Write here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.5. Plot histograms for each of the five data columns (all in one figure). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Now, conduct a PCA on all 9 weather attributes  and select the two most principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def pca_model(X):\n",
    "    \n",
    "    # code here\n",
    "\n",
    "    pca = PCA(n_components=2)\n",
    "    pca.fit(X)\n",
    "    return pca\n",
    "\n",
    "# build X\n",
    "# X is a 2d matrix, where rows are the number \n",
    "# of samples and the columns are the 9 weather \n",
    "# related continuous features.\n",
    "\n",
    "pca = pca_model(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Retain only two columns of the transformed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now transform the data \n",
    "X_2col = pca.transform(X)\n",
    "\n",
    "# code to display or visualize the two columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Next, visualize the transformed data in two dimensions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1. Draw a scatter plot using the two dimensions. Use four different colors for each datapoint, one for each severity category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def pca_plotter(X, severity, s_colors):\n",
    "    \"\"\"\n",
    "    X is the data to be transformed.\n",
    "    severity is the data column containing severity information\n",
    "    s_colors is a list containing the colors to assign each severity value.\n",
    "    \n",
    "    \"\"\"\n",
    "    # code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2 What differences do you observe for the two or three datasets (from answers to Q1.1, Q1.2 and/or Q1.3) after dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# code here\n",
    "\n",
    "# X_q1 = PCA transformed data after Q1.1\n",
    "# X_q2 = PCA transformed data after Q1.2\n",
    "# X_q3 = PCA transformed data after Q1.3 if you answered the bonus question.\n",
    "\n",
    "for x in [X_q1, X_q2, X_q3]:\n",
    "    pca_plotter(x, severity, s_colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr size=\"6\" style=\"color:#d65828;background-color:#d65828;border:none;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (50 pts)Part 2: Visualizations and Explorations of Unstructured Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text data is hard to work with and require special tools to analyze and visualize. We will analyze text corpora using parsers and tokenizers. Then, using word clouds we will visualize the distributions of word tokens in  perceptually meaningful way. Thus, we will then be able to extract and glean patterns from unstructured text data. We will use this data collection for this part of the laboratory.\n",
    "\n",
    "https://www.kaggle.com/stackoverflow/pythonquestions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (25 pts) Question 1: Create a word cloud for tags and answer the following questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please visit  the following data source: https://www.kaggle.com/stackoverflow/pythonquestions#Tags.csv.\n",
    "This data consists  full text of all questions and answers from Stack Overflow that are tagged with the python tag. This data is useful for developing methods of natural language processing analytic methods pertaining to “Q&A” and community  response.   \n",
    "\n",
    "It is normally of interest to find determine a variety of words (or tags)  which either occur frequently or occur very rarely. Our goal is to find those words and also accord those tags/words a measure of strength. To weigh each word’s strength:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Use frequency counts for each tag to highlight most frequent tags. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/Tags.csv\")\n",
    "\n",
    "tags = # code here to extract tag as a list\n",
    "\n",
    "def freq_counter(tags):\n",
    "    # code here\n",
    "    pass\n",
    "\n",
    "d = freq_counter(tags):\n",
    "# d = dict where keys are the tags, \n",
    "# and values are its corresponding frequency counts.\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Collect the results in a table manifest as `alltags.csv` file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# code here to convert to dataframe save csv file\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Plot the histogram of the tags. Choose an appropriate number to plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def plot_histogram(freq_dict):\n",
    "    # code here\n",
    "    \n",
    "\n",
    "plot_histogram(freq_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. What distribution would best describe the occurence of tags ? What do you observe ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Write here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. Now use inverse of frequency counts (1/f) to highlight rarely occurring tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def inverse_freq_counter(tags):\n",
    "    # code here\n",
    "\n",
    "invf = inverse_freq_counter(tags)\n",
    "\n",
    "plot_histogram(invf)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6. Collect the results in a table manifest as a `mostfrequent.csv` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (25 pts) Question 2: Use word clouds to answer the following questions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find other two associated data necessary for the second part of this laboratory.\n",
    "https://www.kaggle.com/astackoverflow/pythonquestions#Answers.csv;\n",
    "https://www.kaggle.com/stackoverflow/pythonquestions#Questions.csv.\n",
    "Here you will tokenize and extract frequency counts. Make sure to also remove stop words. These frequently occur in English and include articles of speech, tense, etc. (e.g., _“an”, “a”, “the”, “is”, “at”,_ etc.). Typically in text pre-processing stop words are filtered out before the actual processing of any natural language text.\n",
    "\n",
    "A typical word cloud shows all the words in a collection placed in such a way that the frequently occurring words appear prominent (large, colorful, etc.). Word clouds can help identify trends and patterns that would otherwise be unclear or difficult to see in a tabular format.  Visit https://en.wikipedia.org/wiki/Tag_cloud for some explanation.\n",
    "\n",
    "> Use `nltk` ([pyPI](https://pypi.org/project/nltk/))([github](https://github.com/nltk/nltk))([docs](https://www.nltk.org/)) for tokenization and general text processing needs.\n",
    ">\n",
    "> Make use of the following python package `wordcloud` ([pyPI](https://pypi.org/project/wordcloud/))([github](https://github.com/amueller/word_cloud))([docs](https://amueller.github.io/word_cloud/index.html)) to do all the heavy lifting with the actual visualization aspects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Build a wordcloud for the python tags and compare the visualization with the histogram. Do you observe any significant differences between the two types of visualizations? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "    \n",
    "def plot_word_cloud(freq_dict):\n",
    "    # code here\n",
    "    wc = WordCloud(background_color=\"black\")\n",
    "    wc.generate_from_frequencies(freq_dict)\n",
    "    \n",
    "\n",
    "# run this to plot them both\n",
    "plot_histogram(freq_dict)\n",
    "plot_word_cloud(freq_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. What are the most frequent words in `Questions.csv`? What are the most frequent word in `Answers.csv` ? Plot adjacent wordclouds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "\n",
    "ans_df = pd.read_csv(\"./data/Answers.csv\")\n",
    "ques_df = pd.read_csv(\"./data/Questions.csv\")\n",
    "\n",
    "def text_freq_counter(texts):\n",
    "    # code here\n",
    "    \n",
    "    # be sure to remove all stop words in questions and answer data\n",
    "    from nltk.corpus import stopwords\n",
    "    print(set(stopwords.words('english')))\n",
    "\n",
    "# Generate word cloud here\n",
    "plot_word_cloud(freq_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. What are the most frequent words combined (StackOverflow Questions + Answers)? Comment on the differences you observe between the three!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Your Comments here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Assign a different color to words depending on where the belong. Assign shades of red for words that only exist in StackOverflow Questions, blue for only StackOverflow Answers and purple for the combined. What do you observe?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Write your observations here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. Do you find the same set of words appear  as highly frequent in all three collections? If yes, highlight them. Describe how you picked and decided on which set of words would be deemed as highly frequent. What criteria did you use ? How many did you choose to display ? Why ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Your description of what you did here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6. Can you find words that are more frequent in StackOverflow Answers but least frequent in StackOverflow Questions? What about visa versa ? What can you say about the differences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Write here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr size=\"6\" style=\"color:#d65828;background-color:#d65828;border:none;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Appendix A\n",
    "\n",
    "To decide how to handle missing data, we must first better understand why exactly are they missing and what could be the causes. Next, we can deploy some strategies to handle missing data. For more information, please peruse this manuscript. Also, some introduction can be found on this Wiki page (https://en.wikipedia.org/wiki/Missing_data). \n",
    "### Mechanics of Missing Data: \n",
    "\n",
    "> (Rubin, Donald B. _\"Inference and missing data.\"_ Biometrika 63.3 (1976): 581-592.) \n",
    "\n",
    "There are some well understood mechanisms or assumptions you can make of your missing data to make an informed decision on how to handle your missing data:\n",
    "1. **Missing Completely At Random(MCAR):** If an attribute has missing data points completely and happens at  random, it can then be assumed that the  probability of an attribute missing its value  is uniformly the same for every sample. For example, everytime the dice rolls $6$, do not enter the value. In such scenarios, if missing data $< 5\\%$, removing the samples containing missing data will not bias your inference. Since the observed distribution is the same as unobserved, if the missing data is more substantial ($5-40\\%$), one impute using linear (numerical data) or logistic (categorical) regression.\n",
    "\n",
    "2. **Missing At Random(MAR):** In reality nothing is truly random, a variable is usually dependent or conditional on existing attributes. This dependency is often hidden and cannot be easily discerned. For example, if blood pressure data are missing at random, conditional on age, gender and ethnicity, then the distributions of missing and observed blood pressures will be similar among people of the same age, gender and ethnicity. Thus, the probability of data being missing is somewhat dependent on other attributes and is not truly random. In such cases, if the missing data is smaller proportionally, we can simply remove those. If however they are substantial, we can interpolate using linear or logistic regression against the conditional attributes.\n",
    "\n",
    "\n",
    "3. **Missing Not At Random (MNAR):** When the data is neither MCAR or MAR and when the data depends on the value itself, it is harder to fix. For example, individuals rarely provide their weight or their age given personal apprehensions and cultural stigmas. In these cases missing data drastically affects the distribution and hence the final inference of the data. Bias and inaccuracies are common and the popular press is rife with such mishaps. The best technical solution is to model the missing data along with your responses or targets.\n",
    "\n",
    "### Practical Handling of Missing Data: \n",
    "\n",
    "The following are the various ways to deal with missing data\n",
    "\n",
    "1. You can eliminate rows that contain the missing data point. If you are working with missing data that occurs $< 5\\%$, and you know the missingness is due to complete randomness(**MCAR**) or is conditionally random (**MAR**), then you can choose to simply eliminate the rows.\n",
    "\n",
    "2. You can also choose to eliminate columns if you think the attribute will not provide any useful information in your analysis. This approach can also be taken if you are missing a substantial amount of data ($50$ to $60 \\%$ or greater) and that including this data attribute will heavily bias your inference.\n",
    "\n",
    "3. Estimate missing values \n",
    "\n",
    "  1. In the case of **categorical data**, you can choose to use Logistic Regression to predict the missing classes/labels. However this will only work if the reason for missingness is **MCAR** or **MAR**. In the case of **MNAR** you cannot use existing predictors and many of the missing classes will not have sufficient ground truth. In such cases, you may have to jointly learn the missing value along with the missingness.\n",
    "\n",
    "  2. In the case of **continuous data**, you can choose to use simple aggregate measures like mean, median or mode as those typically do not affect the overall distribution of the attribute. However that only works when the values missing are not substantial ($< 5\\%$). If larger number of missing values is the case,  you can apply linear regression using existing predictors (use all in case of **MCAR** or use feature subset selection in addition to linear regression for **MAR**). \n",
    "\n",
    "4. Another simpler strategy is to use random sampling based on probability distribution of given data. This works really well when you know the missingness is due to **MCAR**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr size=\"6\" style=\"color:#d65828;background-color:#d65828;border:none;\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
